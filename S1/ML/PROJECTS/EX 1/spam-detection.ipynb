{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy import *\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parsing text\n",
    "this func turns a given string into a list of lowercase words without punctuation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textParse(email: str):\n",
    "    return re.sub(\"[^\\w\\s]\", \"\", email).replace(\"Subject\", \"\", 1).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creates a list of distinct words used in an emails dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'account']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createVocabList(dataSet: list):\n",
    "    vocab_words_spam = []\n",
    "    for sentence in dataSet:\n",
    "        for word in textParse(sentence):\n",
    "            vocab_words_spam.append(word)\n",
    "\n",
    "    return list(dict.fromkeys(vocab_words_spam))\n",
    "\n",
    "createVocabList(['send us your password', 'review our website', 'send your password', 'send us your account'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the dataset (CSV)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m spamss \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspam-ham/spam\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      3\u001B[0m spamss\u001B[38;5;241m.\u001B[39mhead()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# dfs = pd.read_csv('spam_ham_dataset.csv')\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Y = dfs['label_num']\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# spams_train = X_train['text'][X_train['label'] == 'spam']\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# hams_train = X_train['text'][X_train['label'] == 'ham']\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[22], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m spamss \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread() \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspam-ham/spam\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m      3\u001B[0m spamss\u001B[38;5;241m.\u001B[39mhead()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# dfs = pd.read_csv('spam_ham_dataset.csv')\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Y = dfs['label_num']\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# spams_train = X_train['text'][X_train['label'] == 'spam']\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# hams_train = X_train['text'][X_train['label'] == 'ham']\u001B[39;00m\n",
      "File \u001B[1;32mD:\\FST\\MST - AIDS\\S1\\ML\\PROJECTS\\EX 1\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:308\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    304\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m     )\n\u001B[1;32m--> 308\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '1.txt'"
     ]
    }
   ],
   "source": [
    "sppams = []\n",
    "for filename in os.listdir('spam-ham/spam'):\n",
    "     f = open(filename,'r')\n",
    "     sppams.append(f.read())\n",
    "\n",
    "dfs = pd.read_csv('spam_ham_dataset.csv')\n",
    "Y = dfs['label_num']\n",
    "\n",
    "# TODO: install sklearn to split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dfs, Y, test_size=0.2, random_state=1, stratify=Y)\n",
    "\n",
    "spams_train = X_train['text'][X_train['label'] == 'spam']\n",
    "hams_train = X_train['text'][X_train['label'] == 'ham']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails with the word send: 3\n",
      "Prob of the word 'send': 0.6666666666666666 \n",
      "\n",
      "Number of emails with the word us: 2\n",
      "Prob of the word 'us': 0.5 \n",
      "\n",
      "Number of emails with the word your: 3\n",
      "Prob of the word 'your': 0.6666666666666666 \n",
      "\n",
      "Number of emails with the word password: 2\n",
      "Prob of the word 'password': 0.5 \n",
      "\n",
      "Number of emails with the word review: 1\n",
      "Prob of the word 'review': 0.3333333333333333 \n",
      "\n",
      "Number of emails with the word our: 4\n",
      "Prob of the word 'our': 0.8333333333333334 \n",
      "\n",
      "Number of emails with the word website: 1\n",
      "Prob of the word 'website': 0.3333333333333333 \n",
      "\n",
      "Number of emails with the word account: 1\n",
      "Prob of the word 'account': 0.3333333333333333 \n",
      "\n",
      "Number of emails with the word your: 0\n",
      "Prob of the word 'your': 0.2 \n",
      "\n",
      "Number of emails with the word activity: 2\n",
      "Prob of the word 'activity': 0.6 \n",
      "\n",
      "Number of emails with the word report: 1\n",
      "Prob of the word 'report': 0.4 \n",
      "\n",
      "Number of emails with the word benefits: 1\n",
      "Prob of the word 'benefits': 0.4 \n",
      "\n",
      "Number of emails with the word physical: 1\n",
      "Prob of the word 'physical': 0.4 \n",
      "\n",
      "Number of emails with the word the: 1\n",
      "Prob of the word 'the': 0.4 \n",
      "\n",
      "Number of emails with the word importance: 1\n",
      "Prob of the word 'importance': 0.4 \n",
      "\n",
      "Number of emails with the word vows: 1\n",
      "Prob of the word 'vows': 0.4 \n",
      "\n",
      "0.5714285714285714\n",
      "{'send': 0.6666666666666666, 'us': 0.5, 'your': 0.6666666666666666, 'password': 0.5, 'review': 0.3333333333333333, 'our': 0.8333333333333334, 'website': 0.3333333333333333, 'account': 0.3333333333333333}\n",
      "0.42857142857142855\n",
      "{'your': 0.2, 'activity': 0.6, 'report': 0.4, 'benefits': 0.4, 'physical': 0.4, 'the': 0.4, 'importance': 0.4, 'vows': 0.4}\n"
     ]
    }
   ],
   "source": [
    "spam_emails = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "ham_emails = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "\n",
    "def wordsProb(emails: list):\n",
    "    dict_prob = {}\n",
    "    for w in createVocabList(emails):\n",
    "        emails_with_w = 0     # counter\n",
    "        for sentence in emails:\n",
    "            if w in sentence:\n",
    "                emails_with_w+=1\n",
    "\n",
    "        print(f\"Number of emails with the word {w}: {emails_with_w}\")\n",
    "        prob = (emails_with_w+1)/(len(emails)+2)\n",
    "        print(f\"Prob of the word '{w}': {prob} \\n\")\n",
    "        dict_prob[w.lower()] = prob\n",
    "    return dict_prob\n",
    "\n",
    "dict_spamicity = wordsProb(spam_emails)\n",
    "dict_hamicity = wordsProb(ham_emails)\n",
    "\n",
    "\n",
    "prob_spam = len(spam_emails) / (len(spam_emails)+(len(ham_emails)))\n",
    "print(prob_spam)\n",
    "print(dict_spamicity)\n",
    "prob_ham = len(ham_emails) / (len(spam_emails)+(len(ham_emails)))\n",
    "print(prob_ham)\n",
    "print(dict_hamicity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tockenizing test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['renew', 'your', 'password'], ['renew', 'your', 'vows'], ['benefits', 'of', 'our', 'account'], ['the', 'importance', 'of', 'physical', 'activity']]\n"
     ]
    }
   ],
   "source": [
    "tests = ['renew your password', 'renew your vows', 'benefits of our account', 'the importance of physical activity']\n",
    "\n",
    "distinct_words_as_sentences_test = [textParse(sentence) for sentence in tests]\n",
    "\n",
    "print(distinct_words_as_sentences_test)\n",
    "\n",
    "test_spam_tokenized = [distinct_words_as_sentences_test[0], distinct_words_as_sentences_test[1]]\n",
    "test_ham_tokenized = [distinct_words_as_sentences_test[2], distinct_words_as_sentences_test[3]]\n",
    "\n",
    "# test_spam_tokenized = X_test['text'][X_test['label'] == 'spam']\n",
    "# test_ham_tokenized = X_test['text'][X_test['label'] == 'ham']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vect, p1Vect, pC1):\n",
    "    # Calcul de probabilité selon la loi de Bernoulli\n",
    "    p1 = sum(vec2Classify*log(p1Vect)+(1-vec2Classify)*log(1-p1Vect))+log(pC1)\n",
    "    p0 = sum(vec2Classify*log(p0Vect)+(1-vec2Classify)*log(1-p0Vect))+log(1-pC1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "        p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "        testEntry = ['love', 'my', 'dalmation']\n",
    "        thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "        print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)\n",
    "        testEntry = ['stupid', 'garbage']\n",
    "        thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "        print (testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           Testing stemmed SPAM email ['renew', 'your', 'password'] :\n",
      "                 Test word by word: \n",
      "All word probabilities for this sentence: [0.5263157894736842, 0.8163265306122448, 0.7692307692307692]\n",
      "email is HAM: with spammy confidence of 33.04965710980748%\n",
      "0.3304965710980748\n",
      "\n",
      "           Testing stemmed SPAM email ['renew', 'your', 'vows'] :\n",
      "                 Test word by word: \n",
      "All word probabilities for this sentence: [0.5263157894736842, 0.8163265306122448, 0.35714285714285715]\n",
      "email is HAM: with spammy confidence of 15.344483658124902%\n",
      "0.15344483658124902\n",
      "\n",
      "           Testing stemmed HAM email ['benefits', 'of', 'our', 'account'] :\n",
      "                 Test word by word: \n",
      "All word probabilities for this sentence: [0.35714285714285715, 0.5263157894736842, 0.847457627118644, 0.689655172413793]\n",
      "email is HAM: with spammy confidence of 10.985968720749856%\n",
      "0.10985968720749856\n",
      "\n",
      "           Testing stemmed HAM email ['the', 'importance', 'of', 'physical', 'activity'] :\n",
      "                 Test word by word: \n",
      "All word probabilities for this sentence: [0.35714285714285715, 0.35714285714285715, 0.5263157894736842, 0.35714285714285715, 0.2702702702702703]\n",
      "email is HAM: with spammy confidence of 0.6479933977248694%\n",
      "0.006479933977248694\n"
     ]
    }
   ],
   "source": [
    "def mult(list_):\n",
    "    total_prob = 1\n",
    "    for i in list_:\n",
    "         total_prob = total_prob * i\n",
    "    return total_prob\n",
    "\n",
    "def Bayes(email):\n",
    "    probs = []\n",
    "    for word in email:\n",
    "        Pr_S = prob_spam\n",
    "        try:\n",
    "            pr_WS = dict_spamicity[word]\n",
    "        except KeyError:\n",
    "            pr_WS = 1/(len(spam_emails)+2)\n",
    "\n",
    "        Pr_H = prob_ham\n",
    "        try:\n",
    "            pr_WH = dict_hamicity[word]\n",
    "        except KeyError:\n",
    "            pr_WH = (1/(len(ham_emails)+2))\n",
    "\n",
    "        prob_word_is_spam_BAYES = (pr_WS*Pr_S)/((pr_WS*Pr_S)+(pr_WH*Pr_H))\n",
    "        probs.append(prob_word_is_spam_BAYES)\n",
    "    print(f\"All word probabilities for this sentence: {probs}\")\n",
    "    final_classification = mult(probs)\n",
    "    if final_classification >= 0.5:\n",
    "        print(f'email is SPAM: with spammy confidence of {final_classification*100}%')\n",
    "    else:\n",
    "        print(f'email is HAM: with spammy confidence of {final_classification*100}%')\n",
    "    return final_classification\n",
    "for email in test_spam_tokenized:\n",
    "    print('')\n",
    "    print(f\"           Testing stemmed SPAM email {email} :\")\n",
    "    print('                 Test word by word: ')\n",
    "    all_word_probs = Bayes(email)\n",
    "    print(all_word_probs)\n",
    "\n",
    "for email in test_ham_tokenized:\n",
    "    print('')\n",
    "    print(f\"           Testing stemmed HAM email {email} :\")\n",
    "    print('                 Test word by word: ')\n",
    "    all_word_probs = Bayes(email)\n",
    "    print(all_word_probs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
